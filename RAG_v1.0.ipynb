{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T13:47:09.049373Z",
     "start_time": "2024-07-25T13:47:05.802381Z"
    }
   },
   "source": [
    "import os\n",
    "! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py PyMuPDF sentence_transformers python-dotenv"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-nomic in ./venv/lib/python3.12/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: langchain_community in ./venv/lib/python3.12/site-packages (0.2.10)\r\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.12/site-packages (0.7.0)\r\n",
      "Requirement already satisfied: langchainhub in ./venv/lib/python3.12/site-packages (0.1.20)\r\n",
      "Requirement already satisfied: chromadb in ./venv/lib/python3.12/site-packages (0.5.5)\r\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.12/site-packages (0.2.11)\r\n",
      "Requirement already satisfied: langgraph in ./venv/lib/python3.12/site-packages (0.1.14)\r\n",
      "Requirement already satisfied: tavily-python in ./venv/lib/python3.12/site-packages (0.3.5)\r\n",
      "Requirement already satisfied: gpt4all in ./venv/lib/python3.12/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: firecrawl-py in ./venv/lib/python3.12/site-packages (0.0.20)\r\n",
      "Requirement already satisfied: PyMuPDF in ./venv/lib/python3.12/site-packages (1.24.9)\r\n",
      "Requirement already satisfied: sentence_transformers in ./venv/lib/python3.12/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.12/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in ./venv/lib/python3.12/site-packages (from langchain-nomic) (0.2.23)\r\n",
      "Requirement already satisfied: nomic<4.0.0,>=3.0.29 in ./venv/lib/python3.12/site-packages (from langchain-nomic) (3.1.1)\r\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in ./venv/lib/python3.12/site-packages (from langchain-nomic) (10.4.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.12/site-packages (from langchain_community) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.12/site-packages (from langchain_community) (2.0.31)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.12/site-packages (from langchain_community) (3.9.5)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (0.1.93)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (1.26.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.12/site-packages (from langchain_community) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./venv/lib/python3.12/site-packages (from langchain_community) (8.5.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken) (2024.5.15)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.12/site-packages (from langchainhub) (24.1)\r\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./venv/lib/python3.12/site-packages (from langchainhub) (2.32.0.20240712)\r\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.12/site-packages (from chromadb) (1.2.1)\r\n",
      "Requirement already satisfied: pydantic>=1.9 in ./venv/lib/python3.12/site-packages (from chromadb) (2.8.2)\r\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./venv/lib/python3.12/site-packages (from chromadb) (0.7.6)\r\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./venv/lib/python3.12/site-packages (from chromadb) (0.111.1)\r\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.3)\r\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./venv/lib/python3.12/site-packages (from chromadb) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.12/site-packages (from chromadb) (4.12.2)\r\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.12/site-packages (from chromadb) (1.18.1)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./venv/lib/python3.12/site-packages (from chromadb) (0.46b0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.12/site-packages (from chromadb) (0.19.1)\r\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.12/site-packages (from chromadb) (4.66.4)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.12/site-packages (from chromadb) (6.4.0)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.65.1)\r\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb) (4.2.0)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.12/site-packages (from chromadb) (0.12.3)\r\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.12/site-packages (from chromadb) (30.1.0)\r\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb) (4.1.0)\r\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.12/site-packages (from chromadb) (3.10.6)\r\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./venv/lib/python3.12/site-packages (from chromadb) (0.27.0)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./venv/lib/python3.12/site-packages (from langchain) (0.2.2)\r\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in ./venv/lib/python3.12/site-packages (from PyMuPDF) (1.24.9)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (4.42.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (2.3.1)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (from sentence_transformers) (1.5.1)\r\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from sentence_transformers) (1.14.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (0.23.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\r\n",
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.1.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\r\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\r\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in ./venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\r\n",
      "Requirement already satisfied: jinja2>=2.11.2 in ./venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in ./venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\r\n",
      "Requirement already satisfied: email_validator>=2.0.0 in ./venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (2.2.0)\r\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\r\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\r\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\r\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.46->langchain-nomic) (1.33)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (8.1.7)\r\n",
      "Requirement already satisfied: jsonlines in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (4.0.0)\r\n",
      "Requirement already satisfied: loguru in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (0.7.2)\r\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (13.7.1)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (2.2.2)\r\n",
      "Requirement already satisfied: pyarrow in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (17.0.0)\r\n",
      "Requirement already satisfied: pyjwt in ./venv/lib/python3.12/site-packages (from nomic<4.0.0,>=3.0.29->langchain-nomic) (2.8.0)\r\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\r\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\r\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\r\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\r\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.1.0)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: asgiref~=3.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\r\n",
      "Requirement already satisfied: monotonic>=1.5 in ./venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\r\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\r\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./venv/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.12/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-nomic) (3.0.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (2.18.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (0.1.2)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\r\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T13:47:17.411632Z",
     "start_time": "2024-07-25T13:47:17.400308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "7d3e13841504f099",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:28:36.251553Z",
     "start_time": "2024-07-25T21:28:33.772150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load PDF documents\n",
    "pdf_file_paths = [\"./data/Medical Policy FY 23-24.pdf\", \"./data/Provident fund policy.pdf\"]\n",
    "\n",
    "docs_list = []\n",
    "for pdf_path in pdf_file_paths:\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    try:\n",
    "        loaded_docs = loader.load()\n",
    "        docs_list.extend(loaded_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pdf_path}: {e}\")\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splitter = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter and clean metadata\n",
    "filtered_doc = []\n",
    "for doc in doc_splitter:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_doc.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Add to vectorDB\n",
    "embedding = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_doc,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "6c7f249411a9a5c8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:28:39.707391Z",
     "start_time": "2024-07-25T21:28:39.705521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# local_llm = 'gemma'\n",
    "local_llm = 'llama3'\n",
    "# local_llm = 'mistral'"
   ],
   "id": "bd9b6ca631b0997e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained model for semantic similarity\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Define prompt templates\n",
    "context_prompt = PromptTemplate(\n",
    "    template=\"\"\"system You are an assistant for question-answering tasks.\n",
    "    Use the following context to answer the question. Avoid phrases like \"Based on the provided context\". explain the answer in the end. and make a heading with paragraph\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: assistant\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "no_context_prompt = PromptTemplate(\n",
    "    template=\"\"\"system You are an assistant for question-answering tasks.\n",
    "    Answer the question concisely and directly.\n",
    "    Question: {question}\n",
    "    Answer: assistant\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Load a text classification model for future usefulness prediction\n",
    "usefulness_classifier = pipeline(\"text-classification\", model=\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Enhanced Function to check relevance using semantic similarity\n",
    "def is_relevant(question, context_docs, threshold=0.3):\n",
    "    context_text = format_docs(context_docs)\n",
    "\n",
    "    # Split context into smaller chunks\n",
    "    context_chunks = context_text.split('\\n\\n')\n",
    "\n",
    "    # Encode the question\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    max_similarity = 0\n",
    "    for chunk in context_chunks:\n",
    "        # Encode each chunk of context\n",
    "        context_embedding = model.encode(chunk, convert_to_tensor=True)\n",
    "\n",
    "        # Compute similarity\n",
    "        similarity = util.pytorch_cos_sim(question_embedding, context_embedding)\n",
    "        max_similarity = max(max_similarity, similarity.item())\n",
    "\n",
    "    # Return True if similarity is above threshold  \n",
    "    return max_similarity > threshold\n",
    "\n",
    "\n",
    "# Function to determine if a question/statement is useful for the future\n",
    "def is_useful_for_future(question):\n",
    "    # Use the classification pipeline to predict usefulness\n",
    "    result = usefulness_classifier(question)\n",
    "    is_useful = result[0]['label'] == 'USEFUL'\n",
    "    if is_useful:\n",
    "        with open(\"useful_question.txt\", \"a\") as file:\n",
    "            file.write(question + \"\\n\")\n",
    "\n",
    "    return result[0]['label'] == 'IMPORTANT'\n",
    "\n",
    "\n",
    "# Function to get the appropriate chain based on relevance\n",
    "def get_chain(question, context_docs):\n",
    "    if is_relevant(question, context_docs):\n",
    "        print(\"Relevant\")\n",
    "        return context_prompt | llm | StrOutputParser()\n",
    "    else:\n",
    "        print(\"Irrelevant\")\n",
    "        return no_context_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Main function to run the question-answering process\n",
    "def answer_question(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    rag_chain = get_chain(question, docs)\n",
    "    if is_relevant(question, docs):\n",
    "        generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    else:\n",
    "        generation = rag_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Check if the question is_useful useful for the future\n",
    "    if is_useful_for_future(question):\n",
    "        print(f\"This question/statement is useful for the future.\")\n",
    "    else:\n",
    "        print(f\"This question/statement is not particularly useful for the future.\")\n",
    "\n",
    "    return generation\n",
    "\n",
    "\n",
    "# Example question\n",
    "# question = \"What is the vesting period for employer contributions to the provident fund?\"\n",
    "question = \"My dog's illness can be cured by Antihistamines medicines.\"\n",
    "\n",
    "answer = answer_question(question)\n",
    "print(answer)\n"
   ],
   "id": "478f0d441950eb26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T13:47:38.531865Z",
     "start_time": "2024-07-25T13:47:35.476516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "# Load a pre-trained model for semantic similarity\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Define memory\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Enhanced Function to check relevance using semantic similarity\n",
    "def is_relevant(question, context_docs, threshold=0.3):\n",
    "    context_text = format_docs(context_docs)\n",
    "    context_chunks = context_text.split('\\n\\n')\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    max_similarity = 0\n",
    "    for chunk in context_chunks:\n",
    "        context_embedding = model.encode(chunk, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(question_embedding, context_embedding)\n",
    "        max_similarity = max(max_similarity, similarity.item())\n",
    "    return max_similarity > threshold\n",
    "\n",
    "\n",
    "# Function to get the appropriate chain based on relevance\n",
    "def get_chain(question, context_docs):\n",
    "    if is_relevant(question, context_docs):\n",
    "        print(\"Relevant\")\n",
    "\n",
    "        template = \"\"\"system You are an assistant for question-answering tasks.\n",
    "        Use the following context to answer the question. Avoid phrases like \"Based on the provided context\". \n",
    "        Explain the answer in the end and make a heading with paragraph.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer: assistant\"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(input_variables=[\"question\", \"context\"], template=template)\n",
    "\n",
    "        return ConversationChain(\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            memory=ConversationBufferMemory(),\n",
    "        )\n",
    "    else:\n",
    "        print(\"Irrelevant\")\n",
    "\n",
    "        template = \"\"\"system You are an assistant for question-answering tasks.\n",
    "        Answer the question concisely and directly.\n",
    "        Question: {question}\n",
    "        Answer: assistant\"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(input_variables=[\"question\", \"chat_history\"], template=template)\n",
    "\n",
    "        return ConversationChain(\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            memory=ConversationSummaryMemory(llm=llm),\n",
    "        )\n",
    "\n",
    "\n",
    "# Main function to run the question-answering process\n",
    "def answer_question(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    rag_chain = get_chain(question, docs)\n",
    "\n",
    "    if is_relevant(question, docs):\n",
    "\n",
    "        generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    else:\n",
    "        generation = rag_chain.invoke({\"question\": question})\n",
    "\n",
    "    return generation\n",
    "\n",
    "\n",
    "# Example question\n",
    "question = \"What is the vesting period for employer contributions to the provident fund?\"\n",
    "answer = answer_question(question)\n",
    "print(answer)\n"
   ],
   "id": "4e7643eb8556fc13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['context', 'question'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[89], line 87\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# Example question\u001B[39;00m\n\u001B[1;32m     86\u001B[0m question \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the vesting period for employer contributions to the provident fund?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 87\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[43manswer_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28mprint\u001B[39m(answer)\n",
      "Cell \u001B[0;32mIn[89], line 74\u001B[0m, in \u001B[0;36manswer_question\u001B[0;34m(question)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manswer_question\u001B[39m(question):\n\u001B[1;32m     73\u001B[0m     docs \u001B[38;5;241m=\u001B[39m retriever\u001B[38;5;241m.\u001B[39minvoke(question)\n\u001B[0;32m---> 74\u001B[0m     rag_chain \u001B[38;5;241m=\u001B[39m \u001B[43mget_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_relevant(question, docs):\n\u001B[1;32m     78\u001B[0m         generation \u001B[38;5;241m=\u001B[39m rag_chain\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m\"\u001B[39m: docs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: question})\n",
      "Cell \u001B[0;32mIn[89], line 47\u001B[0m, in \u001B[0;36mget_chain\u001B[0;34m(question, context_docs)\u001B[0m\n\u001B[1;32m     38\u001B[0m     template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124msystem You are an assistant for question-answering tasks.\u001B[39m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124m    Use the following context to answer the question. Avoid phrases like \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBased on the provided context\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. \u001B[39m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124m    Explain the answer in the end and make a heading with paragraph.\u001B[39m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124m    Question: \u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124m    Context: \u001B[39m\u001B[38;5;132;01m{context}\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124m    Answer: assistant\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     45\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m PromptTemplate(input_variables\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m\"\u001B[39m], template\u001B[38;5;241m=\u001B[39mtemplate)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mConversationChain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mConversationBufferMemory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIrrelevant\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/development/codeBase/langchain-lamaindex/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:203\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    201\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    202\u001B[0m     emit_warning()\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/development/codeBase/langchain-lamaindex/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:203\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    201\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    202\u001B[0m     emit_warning()\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/development/codeBase/langchain-lamaindex/venv/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[1;32m    339\u001B[0m values, fields_set, validation_error \u001B[38;5;241m=\u001B[39m validate_model(__pydantic_self__\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, data)\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_error:\n\u001B[0;32m--> 341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m validation_error\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     object_setattr(__pydantic_self__, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m'\u001B[39m, values)\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['context', 'question'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:28:49.194443Z",
     "start_time": "2024-07-25T21:28:49.192034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)"
   ],
   "id": "42cb040a57d1d191",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:28:53.120971Z",
     "start_time": "2024-07-25T21:28:53.118654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ],
   "id": "6ddde08a152fbaff",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:29:08.393592Z",
     "start_time": "2024-07-25T21:28:54.862413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation(\"what is my name?\")\n",
    "\n"
   ],
   "id": "1bff8ea294621e9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harisali/development/codeBase/langchain-lamaindex/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name?',\n",
       " 'history': '',\n",
       " 'response': \"I'm happy to help! Unfortunately, I don't have that information stored in our current conversation or any previous interactions we may have had. As a conversational AI, I don't have the ability to access external databases or personal information about individuals without explicit consent. My training data only includes general knowledge and doesn't contain specific details about individual humans. If you'd like to share your name with me, I'm happy to chat with you!\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ],
   "id": "98e5e05b0cf13ffe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation,\n",
    "    \"what is my name?\"\n",
    ")"
   ],
   "id": "e7dd0acfbca177fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(conversation.memory.buffer)\n",
    "\n"
   ],
   "id": "22e129273663d99b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:29:43.964908Z",
     "start_time": "2024-07-25T21:29:43.962122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversationSummary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ],
   "id": "c29fd547ad945ac4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:29:44.633182Z",
     "start_time": "2024-07-25T21:29:44.630983Z"
    }
   },
   "cell_type": "code",
   "source": "print(conversationSummary.memory.prompt.template)\n",
   "id": "d6687f37fcdce19d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T21:29:45.373121Z",
     "start_time": "2024-07-25T21:29:45.352188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversationSummary,\n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ],
   "id": "a47a16d3b8b9b6b8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcount_tokens\u001B[49m(\n\u001B[1;32m      2\u001B[0m     conversationSummary,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhich data source types could be used to give context to the model?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'count_tokens' is not defined"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(conversationSummary.memory.buffer)\n",
    "\n"
   ],
   "id": "2f60072ca3a00aa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "202aee480d0e6da4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
