{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py PyMuPDF sentence_transformers python-dotenv langchain_chroma python-docx\n",
   "id": "a76c0f4ec3e7b621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from docx import Document as DocxDocument"
   ],
   "id": "26df3b179acc78c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# local_llm = 'gemma'\n",
    "# local_llm = 'llama3'\n",
    "local_llm = 'llama3.1'\n",
    "# local_llm = 'mistral'"
   ],
   "id": "eea2da1bf3272441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Directory to check\n",
    "cascade_directory = \"./data/cascade\"\n",
    "policy_directory = \"./data/policy\"\n",
    "\n",
    "directories = [cascade_directory, policy_directory]\n",
    "\n",
    "\n",
    "# Lists to store file paths\n",
    "pdf_file_paths = []\n",
    "docx_file_paths = []\n",
    "\n",
    "# Check for files in the directory\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        print(filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file_paths.append(os.path.join(directory, filename))\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            docx_file_paths.append(os.path.join(directory, filename))\n",
    "\n",
    "docs_list = []\n",
    "\n",
    "# Load PDF files\n",
    "for pdf_path in pdf_file_paths:\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    try:\n",
    "        loaded_docs = loader.load()\n",
    "        docs_list.extend(loaded_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pdf_path}: {e}\")\n",
    "\n",
    "# Load DOCX files\n",
    "for docx_path in docx_file_paths:\n",
    "    try:\n",
    "        docx = DocxDocument(docx_path)\n",
    "        full_text = []\n",
    "        for para in docx.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "        docx_text = '\\n'.join(full_text)\n",
    "        docs_list.append(Document(page_content=docx_text, metadata={\"source\": docx_path}))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {docx_path}: {e}\")\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splitter = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter and clean metadata\n",
    "filtered_doc = []\n",
    "for doc in doc_splitter:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_doc.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Add to vectorDB\n",
    "embedding = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_doc,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ],
   "id": "e56d6d5c8345cd89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "directories",
   "id": "4f018c34a21ca45e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm = ChatOllama(model=local_llm, temperature=0)",
   "id": "e72daeaa52d6ff75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "id": "316640d407f141c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qa_system_prompt = \"\"\"system You are an assistant for question-answering tasks. Use the following context to answer the question. Avoid phrases like \"Based on the provided context\". Explain the answer in the end. and make a heading with paragraph.\n",
    "Question: {input}\n",
    "Context: {context}\n",
    "Answer: assistant\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ],
   "id": "9472f593aac1a996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chat_history = []",
   "id": "d4322da395ccb17d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Explain the significance of document management within Cascadeâ„¢.\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])"
   ],
   "id": "cfb07aa3851e9873",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(ai_msg_1[\"answer\"])",
   "id": "45d2b8a1b233625a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3a95e34edf241e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "id": "a0e4e8b71b62c781",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chat_history",
   "id": "8301e127573750c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for document in ai_msg_1[\"context\"]:\n",
    "    print(document)"
   ],
   "id": "8c805857b967006b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d1aa98fb0966235c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
