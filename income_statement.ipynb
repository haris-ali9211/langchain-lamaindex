{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:41:49.871666Z",
     "start_time": "2024-08-06T13:41:16.970543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in ./venv/lib/python3.9/site-packages (0.3.10)\r\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.9/site-packages (10.4.0)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (2.32.3)\r\n",
      "Requirement already satisfied: langchain_community in ./venv/lib/python3.9/site-packages (0.2.11)\r\n",
      "Requirement already satisfied: langchain_core in ./venv/lib/python3.9/site-packages (0.2.28)\r\n",
      "Requirement already satisfied: langchain_huggingface in ./venv/lib/python3.9/site-packages (0.0.3)\r\n",
      "Collecting llama-index\r\n",
      "  Obtaining dependency information for llama-index from https://files.pythonhosted.org/packages/8a/7c/0591ad0ba69b288a3531220d72ee51c2796209e34dd1a2c3c28a242ff59b/llama_index-0.10.61-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index-0.10.61-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: packaging>=21.3 in ./venv/lib/python3.9/site-packages (from pytesseract) (24.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests) (2024.7.4)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.9/site-packages (from langchain_community) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.9/site-packages (from langchain_community) (2.0.31)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.9/site-packages (from langchain_community) (3.10.1)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./venv/lib/python3.9/site-packages (from langchain_community) (0.6.7)\r\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in ./venv/lib/python3.9/site-packages (from langchain_community) (0.2.12)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./venv/lib/python3.9/site-packages (from langchain_community) (0.1.96)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venv/lib/python3.9/site-packages (from langchain_community) (1.26.4)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./venv/lib/python3.9/site-packages (from langchain_community) (8.5.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.9/site-packages (from langchain_core) (1.33)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./venv/lib/python3.9/site-packages (from langchain_core) (2.8.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.9/site-packages (from langchain_core) (4.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.9/site-packages (from langchain_huggingface) (0.24.5)\r\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in ./venv/lib/python3.9/site-packages (from langchain_huggingface) (3.0.1)\r\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in ./venv/lib/python3.9/site-packages (from langchain_huggingface) (0.19.1)\r\n",
      "Requirement already satisfied: transformers>=4.39.0 in ./venv/lib/python3.9/site-packages (from langchain_huggingface) (4.42.4)\r\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-agent-openai<0.3.0,>=0.1.4 from https://files.pythonhosted.org/packages/c2/9a/9a327ff664e4904b6806f716bf705041d3015b99b12568872833df10b18f/llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\r\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-cli<0.2.0,>=0.1.2 from https://files.pythonhosted.org/packages/fc/74/58a8f8b33bc709947ed08f29055967f49efa995aac59f45d7c4443814d0d/llama_index_cli-0.1.13-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting llama-index-core==0.10.61 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-core==0.10.61 from https://files.pythonhosted.org/packages/69/03/4a925e779805dcc0332283960ffce4d9573b71109b2794f8a9294efdea05/llama_index_core-0.10.61-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_core-0.10.61-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-embeddings-openai<0.2.0,>=0.1.5 from https://files.pythonhosted.org/packages/0b/ee/68b58b7485c82aadd301ae33f1c6071c04ecfccc9c0bdd599a7dd1ee96b4/llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\r\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-indices-managed-llama-cloud>=0.2.0 from https://files.pythonhosted.org/packages/ec/0f/b1d0e685a2994d56bc09382e5933d293f6b158ecc842fd2357d6436b2f37/llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-legacy<0.10.0,>=0.9.48 from https://files.pythonhosted.org/packages/02/0a/0da9d27b0c3b074c1be1151ff4a4558f077c93df463310adfb47d193dde2/llama_index_legacy-0.9.48-py3-none-any.whl.metadata\r\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-llms-openai<0.2.0,>=0.1.27 from https://files.pythonhosted.org/packages/42/38/75ebdb908b250606915ea07dc95002ec23cc7bbc92e33c8608dce961b475/llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\r\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 from https://files.pythonhosted.org/packages/26/18/4c54ba7d2fd833ca93a27503c9069c157c44db6e81739ba583073f2c284d/llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl.metadata (728 bytes)\r\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-program-openai<0.2.0,>=0.1.3 from https://files.pythonhosted.org/packages/ae/6f/f7998c2cfd7de3a33276ed8cabc291291043a45642524c502768d340ebbb/llama_index_program_openai-0.1.7-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\r\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-question-gen-openai<0.2.0,>=0.1.2 from https://files.pythonhosted.org/packages/2d/22/39f3ac5702b0e8ffd4d5a383c7cb2da0eb60f63b95f739345e79b66bf977/llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata\r\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\r\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-readers-file<0.2.0,>=0.1.4 from https://files.pythonhosted.org/packages/b8/dc/2ef5b83e0a1b336e3c7540e12e33fec650993d14abf9a8ca0d6812060cef/llama_index_readers_file-0.1.32-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_index_readers_file-0.1.32-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\r\n",
      "  Obtaining dependency information for llama-index-readers-llama-parse>=0.1.2 from https://files.pythonhosted.org/packages/c7/97/d3a73b62cdef72b1d32527d90f4d32432beb2f48861c8177c5f08d46b974/llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata\r\n",
      "  Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (1.2.14)\r\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.61->llama-index)\r\n",
      "  Obtaining dependency information for dirtyjson<2.0.0,>=1.0.8 from https://files.pythonhosted.org/packages/68/69/1bcf70f81de1b4a9f21b3a62ec0c83bdff991c88d6cc2267d02408457e88/dirtyjson-1.0.8-py3-none-any.whl.metadata\r\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (2024.6.1)\r\n",
      "Requirement already satisfied: httpx in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (0.27.0)\r\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (1.6.0)\r\n",
      "Requirement already satisfied: networkx>=3.0 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (3.2.1)\r\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.61->llama-index)\r\n",
      "  Obtaining dependency information for nltk<4.0.0,>=3.8.1 from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\r\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting openai>=1.1.0 (from llama-index-core==0.10.61->llama-index)\r\n",
      "  Obtaining dependency information for openai>=1.1.0 from https://files.pythonhosted.org/packages/fe/1c/ec0a0618fd7982ff8f11b3d6318e921f7c7677d232fe69fa5fee2b3162b4/openai-1.39.0-py3-none-any.whl.metadata\r\n",
      "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (2.2.2)\r\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (4.66.5)\r\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (0.9.0)\r\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.9/site-packages (from llama-index-core==0.10.61->llama-index) (1.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.3.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./venv/lib/python3.9/site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (0.2.2)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\r\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index)\r\n",
      "  Obtaining dependency information for llama-cloud>=0.0.11 from https://files.pythonhosted.org/packages/6b/71/d158860ad34526d52cdf79de5f0390c84dc170d8ee71c17afba004791022/llama_cloud-0.0.12-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_cloud-0.0.12-py3-none-any.whl.metadata (751 bytes)\r\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./venv/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\r\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\r\n",
      "  Obtaining dependency information for pypdf<5.0.0,>=4.0.1 from https://files.pythonhosted.org/packages/3c/60/eccdd92dd4af3e4bea6d6a342f7588c618a15b9bec4b968af581e498bcc4/pypdf-4.3.1-py3-none-any.whl.metadata\r\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\r\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\r\n",
      "  Obtaining dependency information for striprtf<0.0.27,>=0.0.26 from https://files.pythonhosted.org/packages/a3/cf/0fea4f4ba3fc2772ac2419278aa9f6964124d4302117d61bc055758e000c/striprtf-0.0.26-py3-none-any.whl.metadata\r\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\r\n",
      "  Obtaining dependency information for llama-parse>=0.4.0 from https://files.pythonhosted.org/packages/98/21/1702c91141c0c06692fde4305b873f06ff1649f622666d6be8fbc7da03aa/llama_parse-0.4.9-py3-none-any.whl.metadata\r\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.9/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\r\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.9/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.9/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.7.24)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.9/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\r\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.9/site-packages (from httpx->llama-index-core==0.10.61->llama-index) (4.4.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx->llama-index-core==0.10.61->llama-index) (1.0.5)\r\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from httpx->llama-index-core==0.10.61->llama-index) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.61->llama-index) (0.14.0)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.61->llama-index) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.61->llama-index) (1.4.2)\r\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core==0.10.61->llama-index)\r\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\r\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\r\n",
      "  Obtaining dependency information for greenlet!=0.4.17 from https://files.pythonhosted.org/packages/0b/8a/f5140c8713f919af0e98e6aaa40cb20edaaf3739d18c4a077581e2422ac4/greenlet-3.0.3-cp39-cp39-macosx_11_0_universal2.whl.metadata\r\n",
      "  Using cached greenlet-3.0.3-cp39-cp39-macosx_11_0_universal2.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.61->llama-index) (1.0.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.9/site-packages (from pandas->llama-index-core==0.10.61->llama-index) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas->llama-index-core==0.10.61->llama-index) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas->llama-index-core==0.10.61->llama-index) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio->httpx->llama-index-core==0.10.61->llama-index) (1.2.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.61->llama-index) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\r\n",
      "Downloading llama_index-0.10.61-py3-none-any.whl (6.8 kB)\r\n",
      "Downloading llama_index_core-0.10.61-py3-none-any.whl (15.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.5/15.5 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\r\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\r\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\r\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\r\n",
      "Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\r\n",
      "Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl (11 kB)\r\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.8-py3-none-any.whl (5.9 kB)\r\n",
      "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\r\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\r\n",
      "Downloading llama_index_readers_file-0.1.32-py3-none-any.whl (38 kB)\r\n",
      "Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\r\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\r\n",
      "Downloading llama_cloud-0.0.12-py3-none-any.whl (169 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m169.4/169.4 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\r\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "Downloading openai-1.39.0-py3-none-any.whl (336 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m336.7/336.7 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m295.8/295.8 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\r\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "Using cached greenlet-3.0.3-cp39-cp39-macosx_11_0_universal2.whl (269 kB)\r\n",
      "Installing collected packages: striprtf, dirtyjson, pypdf, nltk, greenlet, distro, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\r\n",
      "Successfully installed dirtyjson-1.0.8 distro-1.9.0 greenlet-3.0.3 llama-cloud-0.0.12 llama-index-0.10.61 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.61 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.27 llama-index-multi-modal-llms-openai-0.1.8 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.32 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 nltk-3.8.1 openai-1.39.0 pypdf-4.3.1 striprtf-0.0.26\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Extracted Text:\n",
      "AMAZON.COM, INC.\n",
      "\n",
      "CONSOLIDATED STATEMENTS OF OPERATIONS\n",
      "(in millions, except per share data)\n",
      "\n",
      "‘Year Ended December 31,\n",
      "\n",
      "2015 2016 2017\n",
      "Net product sales $ 79,268 $ 94,665 §$ 118,573\n",
      "Net service sales 27,738 41,322 59,293\n",
      "Total net sales ~~ 107,006 135,987 177,866\n",
      "Operating expenses:\n",
      "Cost of sales 71,651 88,265 111,934\n",
      "Fulfillment 13,410 17,619 25,249\n",
      "Marketing 5,254 7,233 10,069\n",
      "Technology and content 12,540 16,085 22,620\n",
      "General and administrative 1,747 2,432 3,674\n",
      "Other operating expense, net 171 167 214\n",
      "Total operating expenses 104,773 131,801 173,760\n",
      "Operating income 3933 4,186 4,106\n",
      "Interest income 50 100 202\n",
      "Interest expense (459) (484) (848)\n",
      "Other income (expense), net (256) 90 346\n",
      "Total non-operating income (expense) (665) (294) (300)\n",
      "Income before income taxes 1,568 3,892 3,806\n",
      "Provision for income taxes (950) (1,425) (769)\n",
      "Equity-method investment activity, net of tax (22) (96) (4)\n",
      "Net income S596 8 2371 $ 3,033\n",
      "Basic earnings per share S$ 128 S$ 501 $ 632\n",
      "Diluted earnings per share $ 125 $ 499 $ 615\n",
      "Weighted-average shares used in computation of earnings per share: TT\n",
      "Basic 467 474 480\n",
      "Diluted ne\n",
      "\n",
      "See accompanying notes to consolidated financial statements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pytesseract Pillow requests langchain_community langchain_core langchain_huggingface llama-index\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the image of the income statement\n",
    "image_path = './data/is2.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Use Tesseract to extract text from the image\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "print(\"Extracted Text:\")\n",
    "print(extracted_text)\n",
    "\n",
    "# Parsing the extracted text (This part depends on the format of your income statement)\n",
    "# For demonstration, let's assume the text is formatted in a simple way\n",
    "\n",
    "# def parse_income_statement(text):\n",
    "#     lines = text.split('\\n')\n",
    "#     income_statement = {}\n",
    "#\n",
    "#     for line in lines:\n",
    "#         print(line)\n",
    "#         if 'Revenue' in line:\n",
    "#             income_statement['Revenue'] = float(line.split()[-1].replace(',', ''))\n",
    "#         elif 'Cost of Goods Sold' in line:\n",
    "#             income_statement['Cost of Goods Sold'] = float(line.split()[-1].replace(',', ''))\n",
    "#         elif 'Gross Profit' in line:\n",
    "#             income_statement['Gross Profit'] = float(line.split()[-1].replace(',', ''))\n",
    "#         elif 'Operating Expenses' in line:\n",
    "#             income_statement['Operating Expenses'] = float(line.split()[-1].replace(',', ''))\n",
    "#         elif 'Net Income' in line:\n",
    "#             income_statement['Net Income'] = float(line.split()[-1].replace(',', ''))\n",
    "#\n",
    "#     return income_statement\n",
    "#\n",
    "# parsed_data = parse_income_statement(extracted_text)\n",
    "# print(\"Parsed Data:\")\n",
    "# print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON.COM, INC.\n",
      "\n",
      "CONSOLIDATED STATEMENTS OF OPERATIONS\n",
      "(in millions, except per share data)\n",
      "\n",
      "‘Year Ended December 31,\n",
      "\n",
      "2015 2016 2017\n",
      "Net product sales $ 79,268 $ 94,665 §$ 118,573\n",
      "Net service sales 27,738 41,322 59,293\n",
      "Total net sales ~~ 107,006 135,987 177,866\n",
      "Operating expenses:\n",
      "Cost of sales 71,651 88,265 111,934\n",
      "Fulfillment 13,410 17,619 25,249\n",
      "Marketing 5,254 7,233 10,069\n",
      "Technology and content 12,540 16,085 22,620\n",
      "General and administrative 1,747 2,432 3,674\n",
      "Other operating expense, net 171 167 214\n",
      "Total operating expenses 104,773 131,801 173,760\n",
      "Operating income 3933 4,186 4,106\n",
      "Interest income 50 100 202\n",
      "Interest expense (459) (484) (848)\n",
      "Other income (expense), net (256) 90 346\n",
      "Total non-operating income (expense) (665) (294) (300)\n",
      "Income before income taxes 1,568 3,892 3,806\n",
      "Provision for income taxes (950) (1,425) (769)\n",
      "Equity-method investment activity, net of tax (22) (96) (4)\n",
      "Net income S596 8 2371 $ 3,033\n",
      "Basic earnings per share S$ 128 S$ 501 $ 632\n",
      "Diluted earnings per share $ 125 $ 499 $ 615\n",
      "Weighted-average shares used in computation of earnings per share: TT\n",
      "Basic 467 474 480\n",
      "Diluted ne\n",
      "\n",
      "See accompanying notes to consolidated financial statements.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageDocument' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 14\u001B[0m\n\u001B[1;32m      9\u001B[0m documents \u001B[38;5;241m=\u001B[39m SimpleDirectoryReader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[1;32m     13\u001B[0m embedding \u001B[38;5;241m=\u001B[39m HuggingFaceEmbeddings(model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrag-chroma\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m retriever \u001B[38;5;241m=\u001B[39m vectorstore\u001B[38;5;241m.\u001B[39mas_retriever()\n\u001B[1;32m     22\u001B[0m contextualize_q_system_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mcan you analyze this? only 2015 data\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[0;32m~/Developer/Backend/Python/langchain-lamaindex/venv/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:876\u001B[0m, in \u001B[0;36mChroma.from_documents\u001B[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_documents\u001B[39m(\n\u001B[1;32m    847\u001B[0m     \u001B[38;5;28mcls\u001B[39m: Type[Chroma],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    857\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Chroma:\n\u001B[1;32m    858\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a Chroma vectorstore from a list of documents.\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \n\u001B[1;32m    860\u001B[0m \u001B[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;124;03m        Chroma: Chroma vectorstore.\u001B[39;00m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 876\u001B[0m     texts \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    877\u001B[0m     metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_texts(\n\u001B[1;32m    879\u001B[0m         texts\u001B[38;5;241m=\u001B[39mtexts,\n\u001B[1;32m    880\u001B[0m         embedding\u001B[38;5;241m=\u001B[39membedding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    889\u001B[0m     )\n",
      "File \u001B[0;32m~/Developer/Backend/Python/langchain-lamaindex/venv/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:876\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_documents\u001B[39m(\n\u001B[1;32m    847\u001B[0m     \u001B[38;5;28mcls\u001B[39m: Type[Chroma],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    857\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Chroma:\n\u001B[1;32m    858\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a Chroma vectorstore from a list of documents.\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \n\u001B[1;32m    860\u001B[0m \u001B[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;124;03m        Chroma: Chroma vectorstore.\u001B[39;00m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 876\u001B[0m     texts \u001B[38;5;241m=\u001B[39m [\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpage_content\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    877\u001B[0m     metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_texts(\n\u001B[1;32m    879\u001B[0m         texts\u001B[38;5;241m=\u001B[39mtexts,\n\u001B[1;32m    880\u001B[0m         embedding\u001B[38;5;241m=\u001B[39membedding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    889\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ImageDocument' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "local_llm = 'llama3'\n",
    "llm = ChatOllama(\n",
    "    model=local_llm,\n",
    "    format=\"json\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(extracted_text)\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"can you analyze this? only 2015 data\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:41:53.702393Z",
     "start_time": "2024-08-06T13:41:49.876069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "qa_system_prompt = \"\"\"system You are an assistant for question-answering tasks. Use the following context to answer the question. Avoid phrases like \"Based on the provided context\". Explain the answer in the end. and make a heading with paragraph.\n",
    "Question: {input}\n",
    "Context: {context}\n",
    "Answer: assistant\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "chat_history = []\n",
    "\n",
    "question = \"Analyze this income statement.\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "print(ai_msg_1[\"answer\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
